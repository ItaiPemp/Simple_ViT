# Vision Transformer Implementation
<img width="579" alt="image" src="https://github.com/ItaiPemp/Simple_ViT/assets/102918201/f55993bb-4d8b-4dc1-86be-e8a79fd262cb">


## Overview

This repository contains a simple implementation of the Vision Transformer (ViT) model as described in the original paper titled "An Image is Worth 16x16 Words" by Alexey Dosovitskiy et al. The Vision Transformer is a novel approach to image classification that treats images as sequences of patches and utilizes transformer architectures for learning representations.

## Citation
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al.

```bibtex
@article{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
